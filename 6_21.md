## Learning!

Tuning and improving navigation is not done, and I will continue to work on it for the entirety of my internship here, however I am ready to begin the interesting learning part.

So, right now I have a way to record demonstrations. Currently I am recording costmaps at 1 second intervals and velocity commands as quickly as they come, as well as tf information and odometry just so I can replay things in rviz. After reading a paper (Abbeel & Ng 2004) on apprenticeship learning, I am beginning to understand one approach to this problem. First, we consider our world to be markov, where our markov state is the pose and velocity of our robot, and the occupany grid of our local costmap. We assume that the demonstrator is a utility optimizing agent, and that an ideal policy is one that optimizes the reward function underlying the demonstrators behavior. This means we want to be the best at what the demonstrator is trying to do, which does not necessarily mean mimicing the demonstrations exactly. We simple take demonstrations as good but not neccesarily optimal policies. Furthermore, learning the underlying reward function means we can (hopefully) generalize to state for which we do not have a demonstration.

## Extensions

it would be easy to add moving obstacles to all of this. Our markov state now incluse the position and velocity of moving obstacles, and our feature vector now contains that information.
