## How high should I aim?

So one of the major things holding me back so far is "how high should I aim?". I think of myself as a rather pragmatic developer. I don't want to make anything more complicated than it has to be. Part of learning that skill is noticing simplifications in a system. In my research for instance, I could make an online or mobile interface for performing the intervention demonstration. But technically, I'm not evaluating the effect of the *interface* on the performance on our navigation or learning. So why bother? Just use rviz or other existing tools, right? These are the kinds of simplifications I am seeing. Here are a few of them I can think of right now:

 - Use rviz and existing ros tools for performing the demonstration *or* writing a web or mobile interface
 - Using hand selected heuristics for learning *or* dynamically learning heuristics.
 - Using `move_base` and the default global/local planners, and learning by improving costmaps *or* using `gmapping` and combine other heuristics to and write my own planner
 - On the line of writing my own trajectory planners, should I write plugins to `nav_core` *or* should I make it isolated?
 - Should I do some of the experiments in simulation *or* all of them *or* none of them?
 - On the topic of simulation, do I need to use the vector robot *or* can I just use turtlebot

## Metrics!

I think one reasonable over all metric should be number of interventions per hour. I'm imagining at the end of this summer I'll be running the system for a few hours at a time over a few days. This would give a good indicator if the system is improving at all. But I also need to consider how optimal the paths are. A really successfully method with 99% accuracy might make the routes much longer or slower or something.


## Ok I need to **DO** something.

I'm getting anxious about not having made any real progress towards something I can test, so I'm just gonna dive head first with a simulation turtle bot demo.

 1. Firt I'm going to write a intervention-teleop system.
 1. I will pick 10 spots around my simulated world and send goals via RViz
 1. I will write a `adaptive_costmap` node that adds cost to places that it gets stuck

### SIDE NOTE:

So as I wrote that last bullet, I realized I'm still very confused. Even if I write a SMHA-Star, all that does is help me plan a better trajectory. That doesn't inherently mean there will be any **learning**. Learning needs to come from something happening when the robot gets stuck. The two kinds of somethings I mentioned so far are labeling by the demonstrator, or something automatic. One easy automatic thing is just "blindly avoid this area". I'll start with that for now, but we can do better. Either the robot can detect features about it, or the demonstrator can label features. But when if we know **why** the robot got stuck, or rather if we know something about the area we got stuck in, we might be able to learn better. For instance, we could know that at lunchtime you should avoid this area, or at night you should avoid this area. Or, if you see **XYZ** via some kind of sensing (perception, laser data) you should avoid it.

### Wait... Do I need dynamic obstacle avoidance???

I think we need specific behavior for avoiding people....

Someone must have done this already. Right?
Can I call this out of scope?

Ok wait it seems like AMCL works with dynamic obstacles reasonably well. Dynamic obstacle avoidance is it's own research, so I'm not gonna attempt to solve that. Whatever local planner I use should be reasonably capable.
