## Strafing recovery behavior

the `straf_recovery` plugin is making progress. I've come up with a few reasonable conditions for which direction and how far to move for. But it makes me realize that I'm going to probably spend at least two days getting it working and tested. Which means I won't have made any visible progress towards the learning aspect of this project when Sonia comes back. I feel like I can do some of that without this strafing behavior, since nav is currently pretty well tuned. My thoughts from yesterday proved that a simple "avoid places you get stuck" is not LfD in a true sense. That's ok, but I should just keep in mind that there **are** ways to learn from the demonstration itself. But anyways, if I want to also include some kind of "avoid places you get stuck", I think a simple but modular way to do it is to add a cost function plugin that tracks over time areas where we get stuck and reasons about how costly they are.

## Reasoning about the cost

So one thing I want to do is define a cost function that takes the history of failures, and the state of the world at that time, and adds additional cost to certain areas/plans. If I am simply writing a cost function, then it's not possible to specify directional cost. As in, I can't say "it's ok to go here facing North, but not West". This is something I think would be useful to have, since the direction you move through a point in space greatly impacts whether you are going to get stuck. That kind of informatipn would have to be considered at the level of the global planner. But to start, we can simply add cost to those areas. So when you get stuck, you should set the point to LETHAL_COST. The costmap will then inflate it, and you should hopefully avoid that are in the future. The question then becomes "what happens when lots of those accumulate? You'll get really weird paths". To deal with this, I propose a confidence metric for each obstacle. The confidence controls the decay of the cost. So over time, you forget about points of failure that only happened once. Every time we get stuck there, we reset the cost to LETHAL_COST and increase our confidence.

Ok, so that's a simple way we can hope to avoid areas that cause failure. But as I've said several times, that isn't actually very intelligent. What we really want is the ability to recover from novel failures based on the demonstration of similiar failures.

## True LfD, and learning recovering behaviors

A more intersting and perhaps robust approach is to use the actions taken during the demonstrations to learn how to recover in novel situations. As described Robot Programming by Demonstration (Schall et al), there are two kinds of Lfd. One which learns a non-linear mapping from sensory and motor information, and one that decomposes a skill in a sequence of action-perception units. I think the first one makes more sense here. This is reminding me a lot of the Maximum Margin Planning paper by Ratliff et al., which uses MDP's and a fast "algorithm to compute the minimum of the los-augmented cost function" (whatever the heck that is). The point is that we have a very similar setup, where the demonstrator shows a good path, and attempt to "learn the mapping from features to cost functions so an optimal policy in a MDP with this cost function imitates the expert's behavior". One of the tricks here is that in order to do this, we need "features". Unlike in that work, where they already have the ability to extract many features from satillite maps, we don't have anything like that currently. So, we'd first have to find a way to extract features.

Ya I think I need to talk to sonia about this
