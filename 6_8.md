## Plan for today:

 1. Demonstrate MHA-Star with a console example -- no robots just yet.
 1. Start writing a straffing recovery behaviro
 1. Work on reproducing vivian's results with vector simulation

## Thoughts from this past day on the project

So I haven't done a brain dump in a little while, so here we go...

Everytime I take a break from something or my mind is blank, I try to remind myself of the goal here. The goal is to improve navigation. The goal is to take a system that is *mostly* but not fully autonomosu and make it fully autonomous. The naive approach is to keep trying to tune the existing navigation stack and get better autonomy. This only works up to a point, takes a long time, and yeilds deminishing return. There are many other approaches to improving navigation. It seems there is often a trade off between approaches that generalize well, and approaches to yeild optimal results. We could spend a lot of time coming up with good techniques for navigating the second floor of the IC building, but those approaches would likely not work in another building. Or, we could develop a very generic solution that may not achieve the same performance, but works in many buildings. Our approach is a Learning from Demonstration approach. The idea is that when navigation fails (the definition of failure may be domain specific, and I haven't fully decided what definition I will be using) an expert teleoperator provides correction. Research shows that export demonstrators can easily manipulate the environment to what the robot expects [cite the thing vivian mentioned in her proposal talk]. The goal is to use information about this interaction to improve navigation.

There seem to be three routes for what "use information about this interaction" means. One is to use path information, another is to use perception or sensing information, and the third is to use demonstrator labeled information. By path information, I mean look at the path the demonstrator gave as a source for correction. By preception/sensing I mean look at your pose, any features vision can parse, or other sensory details. By labeled information, I mean having the demonstrator say things like "you drove too close to this wall" or "this is a door and you may find it open or closed". Each of these methods has promise. I'm interested in all of them, but I surely won't have time to look at all of them in depth. One of the simpliest to use is simply position of the robot upon failure. This probably isn't a great way to do it, but it is easy, and therefore with an initial investigation. In other words, if you repeatedly fail in a certain location, you should avoid that location.

So, as a first demo we want to give the robot a series of start and end goals. We want to note locations of repeated failure, and avoid them in the future. In term of actually implementing this using the ROS nav stack, there are a few challenges. First, the question becomes which level of planning does this fall under. Is it the job of the local planner, or the global planner? For instance, if the points of failure are small and often near doors or corners, do we want the robots to avoid these things from the start, or do we want it do only avoid them if it ends up actually passing within their immediate vicinity? My gut reaction tells me global planner is a safer bet, but this is by no means a quantitative decision. I think both could work, but I am going to start with the global planner.

In terms of algorithms to include our new-found knowledge about tricky spots, there are a few methods I can think of. Easiest would be to just add it as a layer in the cost map and boom, avoided. What I could even do is have it be time based. So when it get stuck it makrs is a possibly an issue, but then if it goes there again and doesn't get stuck then it decreases the cost. This is a very limited approach though, because one spot might be very tricky when coming from one direction but not from the other. This leads to me think maybe we need a vector. So we take the dot-product of our current position vector with the position vector from when we got stuck, and one minus that is the cost. In other words, when we go in the same direction at the some point we have higher likelyhood of getting stuck and if we go through in a new direction. This seems like a little bit smarter. This also seems like something I could implement in the costmap as a new custom layer.

## more nav notes

setting `xy_goal_tolerance` too low means you will spin when you get really near your goal. This is fixed by setting XY tolerance to 0.15 or greater

setting `path_distance_bias` too high in relation to `goal_distance_bias` causes complex unexpected scenarios to fail. For insance, a random new obstacle blocking the global plan's path. The 5:2 ration of path to goal which is default was far too high. I suspect a 5:5 or 5:4 reatio is better overall. The absolute value seems to also matter. Making the values 50 and 40 seems to be drastically different than 5 and 4.
