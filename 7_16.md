# I really am doing Inverse Reinforcement Learning!

The current solution I've reverted to is the adaptive costmaps, where we change the costmap and then use our global planner to behave optimally acoording to that costmap. I've now realized that that's IRL! My understanding of IRL is that there is some unknown reward function that the user knows but the system doesn't. We attempt to learn this reward function, and once we do, we follow a policy of simply behaving optimally according to it. Looking at the second half of that definition, since our global planner users a kind of Dijkstra's algorithm over our costmap, we can consider it to behave optimally. So, we're left with the first part. First, learning the reward function is equivalent to learning the cost function. Second, our costmap is a 1-to-1 function mapping cells to cost. However, in IRL we need to be mapping a markov state which captures all the relevant parameters about our place in the environment. As such, a classic costmap which is based only on the obstacles around it is not sufficient to capture our state. The cost in a cell should be a function of *all* the parameters of our state. One such list of parameters is as follows:
 - [X] all marked squares in static maps
 - [ ] position of the robot
 - [ ] velocity of the robot
 - [ ] time of day
 - [ ] position of moving obstacles identified as people
 - [ ] velocity of moving obstacles identified as people

Currently, only the first parameter is used. If our costmaps considered the other parameters, it would be capable of learning behavior with respect to those parameters.

The one tricky constraint in our domain that is not typically imposed on IRL problems, is that the system must be able to detect when it is not behaving optimally. Only then will it prompt for a demonstration. In other words, we should not ask for a demonstration unless the system has "failed" in some respect. This is a practical constraint, because in a real world system you don't want human intervention (demonstration) to occur unless it is necessary. There is a trade off between lots of human demonstration, and cost. We believe that since the robot very rarely fails completely, that the robot will not receive many demonstrations. Furthermore, it will not receive optimal demonstrations.

In order to learn the how the cost of a cell is a function of our state parameters, we can apply any number of machine learning techniques. Regression, neural networks, KNN, decision trees, ect, could all be used. Let's try to pick one. First, I know that we only have a small number of parameters. It's unlikely that we'd ever wat to consider dozens of factors, since our goal is simply to have reliable navigation behavior.

We could do a linear combination of parameters, where we try to learn the weights for each parameter.
We could split this into partially unsupervised learning. If we alter the costmap a little we mimic exploration behavior

## Assuming we do linear combination

We should make one of the parameters the cost according to the static + inflation layers, and another one be the cost according to the obstacle layer. We can initialize the weights for those paremeters to 1. This was we start off with exactly what we had before any of this learning. All other weights should be initialized to zero. So then the question becomes how do we update the weights?

## Actually implementing this

For practical reasons, let's not try to split out obstacles as a seperate parameter. Let's just leave the initial costs as computed by the layers in the costmap as one parameter with one weight. One thing that just occured to me is: Do we have a weight vector for every cell? Or is the cost for any cell a linear combination of features with the one weight vector? I think there's just one?

So what parameters can I actually use?
 - cost according to the lower layers (static + obstacle + inflation) (Cmap)
 - position of robot (Cp)
 - velocity of robot (Cv)
 - time of day (Ct)

We say the the true cost of a cell is equal to `w * <Cmap, Cp, Cv, Ct>`. We initialize w to `<1, 0, 0, 0>`. We then update the weights in the direction of the error. Error is the difference between the true cost of the cell, and the cost as computed with my current guess of `w`.
