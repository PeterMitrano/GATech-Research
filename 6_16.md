## WAAAAAAAAHHHHHH

Ok, so here are the current issues that I think are fixable within the nav stack:

 - starting off in small spaces
 - entering small spaces like doorways from a sharp angle, such as moving along a wall and then entering a doorway along that wall
 - actually reaching the goal when that requires turning around, or when you're slightly off.

Here are some interesting research questions, including new ones Dave gave me:

 - Can we learn adaptive recovery trajectories from demonstration
 - Can we learn tricky scenarios/locations and plan around them
 - Can we avoid obstacles not in the plane of our rangefinder (I don't like this one. the good solution is more sensors)
 - Can we learn how to navigate better in closer quarters, possibly by adapting goals that aren't reachable
 - Can we learn when and in what ways goals are flexible. If the robot can't exactly get here, but it can get close or in a similiar orientation is that good enough?

The most interesting to me are the first two and the last one.

For the last, I actually think it would be awesome to have the robot find other suitable goals based on demonstration. IE, the robot fails to go here, and the demonstrator sends it somewhere slightly different. Can we learn that those goals are interchangable? We could use labeled static maps to help with this. You could identify all the tables, desks, counters, ect. And consider goals to any spot a candidate for being interchangable with another. Or better yet, don't label and do it only on correction. Or in addition to correction, have the robot identify those things from the map. That seems like a waste. Human labeling is easier.

For the first one, is also very intersting, and probably has the most background work. We store each demonstration as a trajectory and costmap pair. we need to learn from these costmaps whether previously demonstrated trajectories are appropriate. Some self-discovery can be used, as can forward-simulating the trajectory. We could simply store a libraries of these demonstrates trajectories, but that seems unlikely to yield success. A more successful approach would require adapting the trajectories. This means we need to know intelligently search the space around the demonstrated trajectory for a reasonable recovery trajectory.

For the second one, it's really all about classifying areas. But we also have to consider direction, and pose with respect to goal. For istance, you can't avoid a place that is required in order to get to your goal just because there's a small chance you'll get stuck. To implement this second idea, you need a trajectory scoring plugin. I'm not sure what technique I should use for comparing costmaps though.


## Thoughts after reading "Active Learning from Demonstration for Robust Autonomous Navigation"

So I finally realized how to learn cost functions for features based on demonstrations! The trick was the reward/feedback function--I couldn't figure it out. Turns out all you do is run the demonstrated path through your costmap function! You want to minimize the cost of the demonstrated paths on your learned cost functions. It's so simple I feel so stupid. I'm not sure I could easily implement something that does it, but I am at a point where I can actually try. The math may stymie me though.

The one thing that still sucks is I'm not sure we have enough features. In the Maximum Margin Planning paper and in this one, they have many features extracted from satellite photography. I only have a 2d map of obstacles. Of course, I could add to this by labeling the static map so I know where doors and such are. Another interesting idea is how to model dynamic obstacles. In other words, when we identify an obstacle in motion, it should have a different cost from static obstacles. So that puts as at three features, possibly a few more if things other than doors are labeled. This might be enough...?

## What can I do in code

Write a node that controls nav from a intervention perpsective. Sub to move_base/status, odom, and cmd_vel, and detect when the robot is stuck. Then cancel navigation. Then alert me, who will teleop the robot out. Once I am alerted, it will start logging tons of stuff. local & global costmaps, /odom, /teleop/cmd_vel, pose & velocity of nearby moving obstacles, pose of labeled static elements within the costmap (doors, chairs, ect...).

For now I don't know what to do with this info, but here's a priminary idea: Learn cost functions and generate a layer or multiple layers in the costmap, so that navigation will incorporate them in future navigation.

A random thought, it may make a huge difference to apply these learned cost functions always, or only in case of failure.
