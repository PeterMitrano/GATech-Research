# Meeting with Sonia

## Lasers/Localization

I need to tune and improve localization if it makes any of my experiments not work well. It's also just a good thing to do. One option is to make a better map. Another option is to tune the amcl parameters. A third option is to get a second lidar.

#### Build a better map.

This is pretty easy to do, so I should try it

#### Tune AMCL

Last resport

#### Get another lidar

It seems that amcl and gmapping might be able to handle two lidars. If they use the frame_id in the header of the LaserScan message then it should work. I have not found many things *recently* on the subject.

## Other Non-Learning stuff

#### Local planning and behavior.

There are obviously still a ton of obnoxious issues with navigation. First off, it never turns left. Second, it thinks it can't turn when it can (but rotate recovery alows it to turn). Third, we'd have more success if it turned fully before moving forward. Fourth, the stagnation detection isn't flawless, and isn't a great method for failure detection. Also publishing the costmap more frequently might be a good idea. I believe that is possible.

#### Global planning

 The global cost map obstacle layer never decreases, and doesn't understand doors. There should be additional inflation around other things that is just preference, instead of just lethal or not. So obstalces in the global planner are necessary but they should be confidence based. If you see them they should go up to lethal but over time they should decrease


## The data I collected

I need to comb through the examples I collected and do two things.

 1. Identify two or three interesting scenarios where failure is legitimate, and learning the most appropriate solution
 1. Identify uninteresting failures into different categories, and show where they usually occur

## LEARNING STUFFFFFF

So I started talking about inverse reinforcment learning and value functions and sonia was kind alike "woah there, slow down. That's hard. Also neither of us fully understand the pipeline of that strategy". We mentioned a few things. First, we talked about doing an approach that is specific to areas of the environment and doesn't try to generalize. Basically, decrease the weight of areas that the demonsrator visits. This could manifest itself as a final layer in the costmap stacks, where cost is decreased in areas of past demonstrated trajectories. We could then use existing planners to plan on this new costmap. However, this does not use multi-heuristic A-star. The advantage of MHA-star is that it can handle inadmissable heuristics. I'm really not sure that is important here. I feel like we can use the existing planners??

So, let's start with that. Just lower the cost of areas near demonstrations. I really think we need to consider the orientation in this somehow. This would be a casefor a different (MHA) planner. That way we could say the state includes theta, and the heuristic depends on theta.

Now, for fancy stuff. Another approach is to use inverse reinforcement learning. Our feature vector is a flattened version of our costmap. We are trying to learn the reward function that the demonstrator is using. One algorithm for this is decsribed in Apprenticeship learning via IRL (Abbeel & Ng 2004). The input to this algorithm is a feature vector, and many demonstrations. We construct an "estimate of the expert's feature expections". We also need to give a MDP. So we still need some method for solving an MDP. In practice, we can use value iteration with a finite number of steps. The available actions for any state should could be a descretized set of directions and rotations. I'm not sure we can do it in the continuous space.

#### Recording/giving demonstrations

To keep things simple, Sonia said I should do demonstrations by watching the robot so that my demonstration trajectories are more optimal. I also then don't have to worry about lag in rviz or with the robot.
