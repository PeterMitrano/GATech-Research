# Status update

Ok, so yesterday I showed the MHA does produce sane paths, but it doesn't know about obstacles. It also doesn't produce *optimal* paths. They're very very curvy. This is sometimes good and results in a faster path, but sometimes not. I'm wondering if MHA is capable of turning in place without moving forward. It may not be...

My demonstration_layer plugin works! It's functionality is limited to two constant cost values, and there is no smoothing. Also, once a cost is decreased it stays that way forever and doesn't improve. I also need to remap the other lab because it got completely rearranged and it's fucking up localization. Or, maybe that's an opportunity to demonstrate/test? I'm not sure.

Other things:
 - Recovery supervisor now looks at number of instances of recovery behavior, not distance/time. This is a _much_ better method
 - Recovery supervisor still has some bugs where if it fails and you re-send a goal it will sometimes still fail the robot immediately
 - Nav Points is a new thing I created that literally just puts a few fixed arrows for sending goals repeatably.

# Thoughts and contemplations moving forward

So what I realized last night is that I don't have a reproducible failure case. I just don't have one, and making one up seems stupid. So, because recovery behavior is good enough to fix most scenarios, why don't we move to attacking recovery behavior happening in the first place. Instead of waiting for total failure, just wait for one single recovery behavior. However, do not cancel the nav goal on one recovery--wait for two for that (even 3, who cares...). So now when we received just one recovey behavior, we know something might have happened. Notice that I say might. In the case of a moving obstacle, it's obviously ok for the robot to enter recovery behavior. So when recovery behavior happens, I have the _option_ to provide a demonstration. I will only do so if I think there was something wrong with the global plan. The demonstration will be collected as usual, and either costmaps will be updated or heuristics will adapt to account for demonstrations.

The overall logic here is thusly: We're trying to improve the global planner, which right now is a simple dijkstra's algorithm (global_planner/GlobalPlanner). It doesn't know everything about our environment, and it's not perfect. We're trying to improve it over time. Since our demonstrations are given based on the rivz costmap view, we are giving demos with the same information the robot has, so it's not an issue of sensing.

# Misc

 - We need to store the goal associated with demonstrated trajectories.
 - I should make a costmap layer that inflates things more gradually.
 - We need to consider if a demonstration becomes less important over time (what if environment changes nearby?)
